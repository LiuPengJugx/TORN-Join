{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Python Spark SQL Execution\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\",\"8g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"8g\") \\\n",
    "    .getOrCreate()\n",
    "fs = pa.hdfs.connect(host='10.77.110.133', port=9001, user='liupengju')\n",
    "import numpy as np\n",
    "import time\n",
    "from partition_tree import PartitionTree\n",
    "from join_until import JOIN_UNTIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def query_with_parquets(hdfs_path, querytype=0, partition_tree=None,pids=[],print_execution_time=False):\n",
    "    start_time = time.time()\n",
    "    pids = str(set(pids)).replace(\" \", \"\")  # '{1,2,3}'\n",
    "    pars_path = hdfs_path + '/partition_' + pids + \".parquet\"\n",
    "    if querytype == 0:\n",
    "        sql = \"SELECT * FROM parquet.`\" + pars_path\n",
    "    elif querytype == 1:\n",
    "        sql = \"SELECT COUNT(*) FROM parquet.`\" + pars_path\n",
    "    elif querytype == 2:\n",
    "        sql = \"SELECT _c0 FROM parquet.`\" + pars_path\n",
    "\n",
    "    # print(\"generated sql:\", sql)\n",
    "    end_time_1 = time.time()\n",
    "    query_result = spark.sql(sql).collect()\n",
    "    #     query_result = spark.sql(sql) # lazy execution\n",
    "    #     query_time = spark.time(spark.sql(sql).collect())  # there is no .time in pyspark\n",
    "\n",
    "    end_time_2 = time.time()\n",
    "\n",
    "    parquets_path=[hdfs_path + '/partition_' + str(pid) + \".parquet\" for pid in pids]\n",
    "    actual_data_size2=0\n",
    "    for par_path in parquets_path:\n",
    "        fw=fs.open(par_path,'rb')\n",
    "        meta = pa.parquet.read_metadata(fw, memory_map=False).to_dict()\n",
    "        actual_data_size2+=meta['num_rows']\n",
    "        fw.close()\n",
    "    query_translation_time = end_time_1 - start_time\n",
    "    query_execution_time = end_time_2 - end_time_1\n",
    "    # print('query execution time: ', query_execution_time)\n",
    "\n",
    "    if print_execution_time:\n",
    "        print('query translation time: ', query_translation_time)\n",
    "        print('query execution time: ', query_execution_time)\n",
    "\n",
    "    # return (query_result, query_translation_time, query_execution_time) # this takes too much memory\n",
    "    return (query_translation_time, query_execution_time, len(query_result),actual_data_size2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def batch_query(a_training_set,b_training_set,a_training_set_for_join,b_training_set_for_join,A_partition_path,B_partition_path,group_type,hdfs_path_a,hdfs_path_b):\n",
    "    ju = JOIN_UNTIL(None, None, join_attr, len(used_dims))\n",
    "    a_join_queries, b_join_queries = ju.generate_join_queries(a_training_set_for_join, b_training_set_for_join)\n",
    "    partition_tree_a = PartitionTree(len(used_dims))\n",
    "    partition_tree_b = PartitionTree(len(used_dims))\n",
    "    partition_tree_a.join_attr,partition_tree_b.join_attr=0,0\n",
    "    pa_A,pa_B=partition_tree_a.load_tree(A_partition_path),partition_tree_b.load_tree(B_partition_path)\n",
    "    ju.set_partitioner(pa_A, pa_B)\n",
    "    total_a_ids,total_b_ids= ju.print_shuffle_hyper_blocks(a_join_queries, b_join_queries,group_type)\n",
    "    total_query_exection_time,totol_real_data_size=0,0\n",
    "    # record time for normal queries\n",
    "    for i,query_set in enumerate([a_training_set,b_training_set]):\n",
    "        tree=pa_A if i==0 else pa_B\n",
    "        cur_hdfs_path=hdfs_path_a if i==0 else hdfs_path_b\n",
    "        for query in query_set:\n",
    "            pids=tree.query_single(query)\n",
    "            result=query_with_parquets(hdfs_path=cur_hdfs_path,querytype=2,partition_tree=tree,pids=pids)\n",
    "            total_query_exection_time+=result[1]\n",
    "            totol_real_data_size+=result[3]\n",
    "    # record time for join queries\n",
    "    for no,a_ids in enumerate(total_a_ids):\n",
    "        b_ids=total_b_ids[no]\n",
    "        result_a=query_with_parquets(hdfs_path=hdfs_path_a,querytype=2,partition_tree=pa_A,pids=a_ids)\n",
    "        result_b=query_with_parquets(hdfs_path=hdfs_path_b,querytype=2,partition_tree=pa_B,pids=b_ids)\n",
    "        for result in [result_a,result_b]:\n",
    "            total_query_exection_time+=result[1]\n",
    "            totol_real_data_size+=result[3]\n",
    "    print(\"Query execution time:\",total_query_exection_time)\n",
    "    print(\"Access row count:\",totol_real_data_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-77866e363632>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mquery_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'/home/liupengju/pycharmProjects/NORA_JOIN_SIMULATION/NORA_experiments/queryset/join/'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# scale 100\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0ma_training_set_for_join\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenfromtxt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{query_path}a_prob{problem_type}_{scale_factor}_train.csv\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdelimiter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m','\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mb_training_set_for_join\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenfromtxt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{query_path}b_prob{problem_type}_{scale_factor}_train.csv\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdelimiter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m','\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;31m## scale 50 and 10\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# ==== set environment parameters and generate dataset ====\n",
    "scale_factor = 10\n",
    "problem_type = 2\n",
    "query_path = '/home/liupengju/pycharmProjects/NORA_JOIN_SIMULATION/NORA_experiments/queryset/join/'\n",
    "# scale 100\n",
    "a_training_set=np.genfromtxt(f\"{query_path}a_prob{problem_type}_{scale_factor}_train.csv\",delimiter=',')\n",
    "b_training_set=np.genfromtxt(f\"{query_path}a_prob{problem_type}_{scale_factor}_train.csv\",delimiter=',')\n",
    "a_training_set_for_join = np.genfromtxt(f\"{query_path}join_a_prob{problem_type}_{scale_factor}_train.csv\",delimiter=',')\n",
    "b_training_set_for_join = np.genfromtxt(f\"{query_path}join_b_prob{problem_type}_{scale_factor}_train.csv\",delimiter=',')\n",
    "used_dims = [1,2,3]\n",
    "num_dims = 16\n",
    "join_attr=0\n",
    "column_names = ['_c'+str(i) for i in range(num_dims)]\n",
    "column_name_dict = {}\n",
    "for i in range(num_dims):\n",
    "    column_name_dict[i] = column_names[i]\n",
    "# scale 100\n",
    "# hdfs_path_nora = 'hdfs://10.77.110.133:9001/par_nora/NORA/prob'+str(problem_type)+'/scale100/merged/'\n",
    "# hdfs_path_qdtree ='hdfs://10.77.110.133:9001/par_nora/QdTree/prob'+str(problem_type)+'/scale100/merged/'\n",
    "# hdfs_path_paw ='hdfs://10.77.110.133:9001/par_nora/PAW/prob'+str(problem_type)+'/scale100/merged/'\n",
    "\n",
    "# scale 50 and 10\n",
    "hdfs_base_path = 'hdfs://10.77.110.133:9001/par_nora/join/'\n",
    "hdfs_path_nora_a = hdfs_base_path + 'NORA/prob' + str(problem_type) + '/scale' + str(scale_factor) + \"/merged_A/\"\n",
    "hdfs_path_nora_b = hdfs_base_path + 'NORA/prob' + str(problem_type) + '/scale' + str(scale_factor) + \"/merged_B/\"\n",
    "hdfs_path_adaptdb_a = hdfs_base_path + 'AdaptDB/prob' + str(problem_type) + '/scale' + str(scale_factor) + \"/merged_A/\"\n",
    "hdfs_path_adaptdb_b = hdfs_base_path + 'AdaptDB/prob' + str(problem_type) + '/scale' + str(scale_factor) + \"/merged_B/\"\n",
    "\n",
    "# newly added\n",
    "querytype = 2 # 0: SELECT *;  2: SELECT variance(_c0)\n",
    "partition_base_path = '/home/liupengju/pycharmProjects/NORA_JOIN_SIMULATION/PartitionLayout/join/'\n",
    "# scale 100\n",
    "jnora_A_partition_path = partition_base_path + 'prob' + str(problem_type) + '_jnora_A_scale' + str(scale_factor)\n",
    "jnora_B_partition_path = partition_base_path + 'prob' + str(problem_type) + '_jnora_B_scale' + str(scale_factor)\n",
    "adaptdb_A_partition_path = partition_base_path + 'prob' + str(problem_type) + '_adaptdb_A_scale' + str(scale_factor)\n",
    "adaptdb_B_partition_path = partition_base_path + 'prob' + str(problem_type) + '_adaptdb_B_scale' + str(scale_factor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_query(a_training_set,b_training_set,a_training_set_for_join,b_training_set_for_join,jnora_A_partition_path,jnora_B_partition_path,3,hdfs_path_nora_a,hdfs_path_nora_b)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_query(a_training_set,b_training_set,a_training_set_for_join,b_training_set_for_join,adaptdb_A_partition_path,adaptdb_B_partition_path,1,hdfs_path_adaptdb_a,hdfs_path_adaptdb_b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}