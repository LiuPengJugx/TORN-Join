{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_helper import DatasetAndQuerysetHelper\n",
    "from partition_algorithm import PartitionAlgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "used_dims = [1,2]\n",
    "scale_factor=1\n",
    "# block_size = int(1000000/scale_factor)\n",
    "block_size = 10000\n",
    "max_active_ratio=3\n",
    "base_path='/home/liupengju/pycharmProjects/NORA_JOIN_SIMULATION/NORA_experiments'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "helper = DatasetAndQuerysetHelper(used_dimensions=used_dims, scale_factor=scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# helper.generate_dataset_and_save(base_path+'/dataset/lineitem_1.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6001215\n",
      "[[1.e+00 2.e+05]\n",
      " [1.e+00 1.e+04]]\n"
     ]
    }
   ],
   "source": [
    "dataset, domains = helper.load_dataset(used_dims)\n",
    "boundary = [interval[0] for interval in domains] + [interval[1] for interval in domains]\n",
    "print(len(dataset))  # 6001309\n",
    "print(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  20.   50.  100.  200.  500. 1000.]\n"
     ]
    }
   ],
   "source": [
    "query_amount=np.array([0.02,0.05,0.1,0.2,0.5,1])*1000\n",
    "print(query_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Time (s): 1.3159151077270508\n",
      "Build Time (s): 2.0322675704956055\n",
      "Build Time (s): 0.34600830078125\n",
      "Build Time (s): 0.25565457344055176\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-f53c7b04b051>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mpa3\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPartitionAlgorithm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0;31m# pa3.InitializeWithPAW2(training_set, len(boundary) // 2, boundary, dataset, block_size,using_beam_search=True, candidate_size = 2, candidate_depth = 2,max_active_ratio=50,strategy = 0)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m     \u001B[0mpa3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInitializeWithNORA\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_set\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mboundary\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mboundary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_threshold\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mblock_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0musing_beam_search\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcandidate_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcandidate_depth\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0musing_1_by_1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0musing_kd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m     \u001B[0mpa3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartition_tree\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_redundant_partition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mqueries\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining_set\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdata_threshold\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mblock_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pycharmProjects/NORA_JOIN_SIMULATION/partition_algorithm.py\u001B[0m in \u001B[0;36mInitializeWithNORA\u001B[0;34m(self, queries, num_dims, boundary, dataset, data_threshold, using_1_by_1, using_kd, depth_limit, return_query_cost, using_beam_search, using_beam_search_plus, candidate_size, candidate_depth)\u001B[0m\n\u001B[1;32m    161\u001B[0m                 \u001B[0;31m# profile.runcall(self.__NORA_Beam_Search,data_threshold, candidate_size, candidate_depth)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m                 \u001B[0;31m# profile.print_stats()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 163\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__NORA_Beam_Search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_threshold\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcandidate_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcandidate_depth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    164\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0musing_beam_search_plus\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__NORA_Beam_Search_Plus\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_threshold\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mqueries\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcandidate_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcandidate_depth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pycharmProjects/NORA_JOIN_SIMULATION/partition_algorithm.py\u001B[0m in \u001B[0;36m__NORA_Beam_Search\u001B[0;34m(self, data_threshold, candidate_size, candidate_depth)\u001B[0m\n\u001B[1;32m   1000\u001B[0m                                                                          return_query_cost = True)\n\u001B[1;32m   1001\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mright_cost\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1002\u001B[0;31m                     \u001B[0mexplored_cost\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mleft_cost\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mright_cost\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1003\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mexplored_cost\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mmin_cost\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1004\u001B[0m                         \u001B[0mmin_cost\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexplored_cost\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "result={}\n",
    "for query_num in query_amount:\n",
    "    training_set, testing_set = helper.generate_queryset_and_save(int(query_num), queryset_type=1) # 1-1\n",
    "    cost_res = list()\n",
    "    # = = = = = Test PartitionAlgorithm (QDT) = = = = =\n",
    "    pa1 = PartitionAlgorithm()\n",
    "    pa1.InitializeWithQDT(training_set, len(boundary) // 2, boundary, dataset, data_threshold=block_size)\n",
    "\n",
    "    # = = = = = Test PartitionAlgorithm (PAW) = = = = =\n",
    "    pa2 = PartitionAlgorithm()\n",
    "    # pa2.InitializeWithPAW(training_set, len(boundary)//2, boundary, dataset, block_size,using_beam_search=False, candidate_size = 2, candidate_depth = 2, max_active_ratio=50,strategy = 0)\n",
    "    pa2.InitializeWithPAW(training_set, len(boundary) // 2, boundary, dataset, block_size, max_active_ratio=max_active_ratio,strategy=1)\n",
    "\n",
    "\n",
    "    # = = = = = Test PartitionAlgorithm (ME) = = = = =\n",
    "    pa3 = PartitionAlgorithm()\n",
    "    # pa3.InitializeWithPAW2(training_set, len(boundary) // 2, boundary, dataset, block_size,using_beam_search=True, candidate_size = 2, candidate_depth = 2,max_active_ratio=50,strategy = 0)\n",
    "    pa3.InitializeWithNORA(training_set, len(boundary)//2, boundary, dataset, data_threshold = block_size, using_beam_search=True, candidate_size = 2, candidate_depth = 2,using_1_by_1 = True, using_kd = True)\n",
    "    pa3.partition_tree.set_redundant_partition(queries=training_set,data_threshold=block_size)\n",
    "\n",
    "    algos = [pa1,pa2,pa3]\n",
    "    for algo in algos:\n",
    "        cost_res.append(algo.partition_tree.evaluate_query_cost(training_set, True))\n",
    "    results = helper.real_result_size(dataset, training_set)\n",
    "    cost_res.append(sum(results)/len(results))\n",
    "    result[query_num]=np.round(np.array(cost_res)/dataset.shape[0],8)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# search dimensionality\n",
    "helper = DatasetAndQuerysetHelper(scale_factor=scale_factor) # EXAMPLE\n",
    "c_dataset, c_domains = helper.load_dataset()\n",
    "query_num=500\n",
    "result={}\n",
    "available_dims=[0,1,2,4,5]\n",
    "for num_dims in range(2,6):\n",
    "    # used_dims = [i for i in range(1, num_dims+1)]\n",
    "    used_dims=available_dims[:num_dims]\n",
    "    dataset=c_dataset[:,used_dims]\n",
    "    domains=c_domains[used_dims]\n",
    "    boundary = [interval[0] for interval in domains]+[interval[1] for interval in domains]\n",
    "    helper.used_dimensions=used_dims\n",
    "    training_set, testing_set = helper.generate_queryset_and_save(query_num, queryset_type=0)\n",
    "    cost_res = list()\n",
    "   # = = = = = Test PartitionAlgorithm (QDT) = = = = =\n",
    "    pa1 = PartitionAlgorithm()\n",
    "    pa1.InitializeWithQDT(training_set, len(boundary) // 2, boundary, dataset, data_threshold=block_size)\n",
    "\n",
    "    # = = = = = Test PartitionAlgorithm (PAW) = = = = =\n",
    "    pa2 = PartitionAlgorithm()\n",
    "    # pa2.InitializeWithPAW(training_set, len(boundary)//2, boundary, dataset, block_size,using_beam_search=False, candidate_size = 2, candidate_depth = 2, max_active_ratio=50,strategy = 0)\n",
    "    pa2.InitializeWithPAW(training_set, len(boundary) // 2, boundary, dataset, block_size, max_active_ratio=max_active_ratio,strategy=1)\n",
    "\n",
    "    # = = = = = Test PartitionAlgorithm (ME) = = = = =\n",
    "    pa3 = PartitionAlgorithm()\n",
    "    # pa3.InitializeWithPAW2(training_set, len(boundary) // 2, boundary, dataset, block_size,using_beam_search=True, candidate_size = 2, candidate_depth = 2,max_active_ratio=50,strategy = 0)\n",
    "    pa3.InitializeWithNORA(training_set, len(boundary)//2, boundary, dataset, data_threshold = block_size, using_beam_search=True, candidate_size = 2, candidate_depth = 2,using_1_by_1 = True, using_kd = True)\n",
    "    pa3.partition_tree.set_redundant_partition(queries=training_set,data_threshold=block_size)\n",
    "\n",
    "    algos = [pa1,pa2,pa3]\n",
    "    for algo in algos:\n",
    "        cost_res.append(algo.partition_tree.evaluate_query_cost(training_set, True))\n",
    "    results = helper.real_result_size(dataset, training_set)\n",
    "    cost_res.append(sum(results)/len(results))\n",
    "    result[str(used_dims)]=np.round(np.array(cost_res)/dataset.shape[0],8)\n",
    "\n",
    "print(result)\n",
    "# {'[1, 2]': array([0.00604011, 0.00535958, 0.00391104, 0.00254143]), \n",
    "# '[1, 2, 3]': array([0.00314586, 0.00293257, 0.00223305, 0.00010503]), \n",
    "# '[1, 2, 3, 4]': array([2.67546e-03, 2.77061e-03, 2.23621e-03, 4.97000e-06]), \n",
    "# '[1, 2, 3, 4, 5]': array([2.53365e-03, 2.52032e-03, 2.09957e-03, 2.60000e-07])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extended_training_set = helper.extend_queryset(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# helper.visualize_queryset_and_dataset([0,1], training_set, testing_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# distance = helper.queryset_distance(training_set[0:25], training_set[25:])\n",
    "# # save the special queryset\n",
    "# np.savetxt(f\"{base_path}/queryset/prob1/uniform_queries_train.csv\", training_set, delimiter=',')\n",
    "# np.savetxt(f\"{base_path}/queryset/prob1/uniform_queries_test.csv\", testing_set, delimiter=',')\n",
    "# np.savetxt(f\"{base_path}/queryset/prob1/skewed_queries_train.csv\", training_set, delimiter=',')\n",
    "# np.savetxt(f\"{base_path}/queryset/prob1/skewed_queries_test.csv\", testing_set, delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b76f585cb0017b15e5d8539d3843191cbcb7b2b9a503c1b5935eb73a41b9bd63"
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}