{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # this must be executed before the below import\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "import ray\n",
    "import time\n",
    "import rtree\n",
    "from rtree import index\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import threading\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from partition_tree import PartitionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liupengju/anaconda3/envs/torch/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n",
      "/home/liupengju/anaconda3/envs/torch/lib/python3.6/site-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAll([(\"spark.executor.memory\", \"24g\"),(\"spark.driver.memory\",\"24g\"),\n",
    "                           (\"spark.memory.offHeap.enabled\",True),(\"spark.memory.offHeap.size\",\"16g\"),\n",
    "                          (\"spark.driver.maxResultSize\", \"16g\")])\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "import os\n",
    "os.environ['HADOOP_HOME'] = '/home/liupengju/hadoop'\n",
    "os.environ['JAVA_HOME'] = '/home/liupengju/java/jdk1.8.0_281'\n",
    "os.environ['ARROW_LIBHDFS_DIR'] = '/home/liupengju/hadoop/lib/native'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_chunk_row(row, used_dims, partition_tree, pid_data_dict, count, k):\n",
    "    if count[0] % 100000 == 0:\n",
    "        print('proces',k,'has routed',count[0],'rows')\n",
    "    count[0] += 1\n",
    "    row_numpy = row.to_numpy()\n",
    "    row_point = row_numpy[used_dims].tolist()\n",
    "    pids = [0]\n",
    "    try:\n",
    "        pids = partition_tree.get_pid_for_data_point(row_point)\n",
    "    except:\n",
    "        print(row_point)\n",
    "    if isinstance(pids,list):\n",
    "        for pid in pids:\n",
    "            if pid in pid_data_dict:\n",
    "                pid_data_dict[pid]+=[row_numpy.tolist()]\n",
    "            else:\n",
    "                pid_data_dict[pid]=[row_numpy.tolist()]\n",
    "\n",
    "@ray.remote\n",
    "def process_chunk(chunk, used_dims, partition_path, k, partition_tree):\n",
    "    print(\"enter data routing process\", k, '..')\n",
    "    pid_data_dict = {}\n",
    "    count = [0]\n",
    "    chunk.apply(lambda row: process_chunk_row(row, used_dims, partition_tree, pid_data_dict, count, k), axis=1)\n",
    "    dict_id = ray.put(pid_data_dict)\n",
    "    print(\"exit data routing process\", k, \".\")\n",
    "    return dict_id\n",
    "\n",
    "@ray.remote\n",
    "def merge_epochs(parameters):\n",
    "    #fs = pa.hdfs.connect()\n",
    "    pids, epoch_count, hdfs_path, fs, merge_process = parameters\n",
    "    for pid in pids:\n",
    "        parquets = []\n",
    "        for epoch in range(epoch_count):\n",
    "            path = hdfs_path + \"epoch_\" + str(epoch) + '/partition_' + str(pid)+'.parquet'\n",
    "            #print(path)\n",
    "            try:\n",
    "                par = pq.read_table(path)\n",
    "                parquets.append(par)\n",
    "            except:\n",
    "                continue\n",
    "        print(\"process\", merge_process, \"pid\", pid, \" len parquets (epochs):\", len(parquets))\n",
    "        if len(parquets) == 0:\n",
    "            continue\n",
    "        merged_parquet = pa.concat_tables(parquets)\n",
    "        merge_path = hdfs_path + 'merged/partition_' + str(pid)+'.parquet'\n",
    "        with fs.open(merge_path,'wb') as f:\n",
    "            pq.write_table(merged_parquet, f)\n",
    "        # fw = fs.open(merge_path,'wb')\n",
    "        # pq.write_table(merged_parquet, fw)\n",
    "        # fw.close()\n",
    "    print('exit merge process', merge_process)\n",
    "\n",
    "def merge_dict(base_dict, new_dict):\n",
    "    for key, val in new_dict.items():\n",
    "        if key in base_dict:\n",
    "            base_dict[key] += val\n",
    "        else:\n",
    "            base_dict[key] = val\n",
    "    new_dict.clear()\n",
    "\n",
    "def dump_dict_2_hdfs_epoch(merged_dict, column_names, hdfs_path, fs, epoch):\n",
    "    #print('= = = start dumping in main thread = = =')\n",
    "    for pid, val in merged_dict.items():\n",
    "        #print(\"writing to pid:\",pid)\n",
    "        path = hdfs_path + 'epoch_'+ str(epoch) +'/partition_' + str(pid) + '.parquet'\n",
    "        pdf = pd.DataFrame(val, columns=column_names)\n",
    "        adf = pa.Table.from_pandas(pdf)\n",
    "        #fw = fs.open(path, 'wb')\n",
    "        with fs.open(path,'wb') as f:\n",
    "            pq.write_table(adf, f,write_statistics=False,use_dictionary=False,compression='none')\n",
    "        # fw = fs.open(path,'wb') # it seems the new version does not have the open function\n",
    "        # pq.write_table(adf, fw)\n",
    "        # fw.close()\n",
    "    #print('= = = exit dumping = = =')\n",
    "\n",
    "\n",
    "def batch_data_parallel(table_path, partition_path, chunk_size, used_dims, hdfs_path, num_dims, num_process,\n",
    "                        hdfs_private_ip):\n",
    "    begin_time = time.time()\n",
    "\n",
    "    ray.init(num_cpus=num_process)\n",
    "\n",
    "    # column names for pandas dataframe\n",
    "    cols = [i for i in range(num_dims)]\n",
    "    col_names = ['_c' + str(i) for i in range(num_dims)]\n",
    "    # pyarrow parquent append\n",
    "    # fs = pa.fs.HadoopFileSystem(hdfs_private_ip, port=9001, user='hdfs', replication=1)\n",
    "    fs=pa.hdfs.connect(host=hdfs_private_ip, port=9001, user='liupengju')\n",
    "    partition_tree = PartitionTree(len(used_dims))\n",
    "    partition_tree.load_tree(partition_path)\n",
    "\n",
    "    # chunks\n",
    "    chunk_count = 0\n",
    "    epoch_count = 0\n",
    "\n",
    "    # collect object refs\n",
    "    result_ids = []\n",
    "    last_batch_ids = []\n",
    "    first_loop = True\n",
    "\n",
    "    for chunk in pd.read_table(table_path, delimiter='|', usecols=cols, names=col_names, chunksize=chunk_size):\n",
    "        print('reading chunk: ', chunk_count)\n",
    "\n",
    "        chunk_id = ray.put(chunk)\n",
    "        result_id = process_chunk.remote(chunk_id, used_dims, partition_path, chunk_count, partition_tree)\n",
    "\n",
    "        del chunk_id\n",
    "        result_ids.append(result_id)\n",
    "        del result_id\n",
    "\n",
    "        # after all process allocated a chunk, process and dump the data\n",
    "        if chunk_count % num_process == num_process - 1:\n",
    "\n",
    "            if first_loop:\n",
    "                first_loop = False\n",
    "                last_batch_ids = result_ids.copy()\n",
    "                result_ids.clear()\n",
    "                chunk_count += 1\n",
    "                continue\n",
    "            else:\n",
    "                print(\"= = = Process Dump For Chunk\", chunk_count - 2 * num_process + 1, \"to\",\n",
    "                      chunk_count - num_process, \"= = =\")\n",
    "                base_dict = {}\n",
    "                while len(last_batch_ids):\n",
    "                    done_id, last_batch_ids = ray.wait(last_batch_ids)\n",
    "                    dict_id = ray.get(done_id[0])\n",
    "                    result_dict = ray.get(dict_id)\n",
    "                    merge_dict(base_dict, result_dict)\n",
    "                dump_dict_2_hdfs_epoch(base_dict, col_names, hdfs_path, fs,\n",
    "                                       epoch_count)  # consider whether we should use another process\n",
    "                epoch_count += 1\n",
    "                base_dict.clear()\n",
    "                print(\"= = = Finish Dump For Chunk\", chunk_count - 2 * num_process + 1, \"to\", chunk_count - num_process,\"= = =\")\n",
    "                last_batch_ids = result_ids.copy()\n",
    "                result_ids.clear()\n",
    "\n",
    "            current_time = time.time()\n",
    "            time_elapsed = current_time - begin_time\n",
    "            print(\"= = = TOTAL PROCESSED SO FAR:\", (chunk_count - num_process + 1) * chunk_size, \"ROWS. TIME SPENT:\",time_elapsed, \"SECONDS = = =\")\n",
    "\n",
    "        chunk_count += 1\n",
    "\n",
    "    # process the last few batches\n",
    "    print(\"= = = Process Dump For Last Few Chunks = = =\")\n",
    "    base_dict = {}\n",
    "    while len(last_batch_ids):\n",
    "        done_id, last_batch_ids = ray.wait(last_batch_ids)\n",
    "        dict_id = ray.get(done_id[0])\n",
    "        result_dict = ray.get(dict_id)\n",
    "        merge_dict(base_dict, result_dict)\n",
    "    dump_dict_2_hdfs_epoch(base_dict, col_names, hdfs_path, fs, epoch_count)\n",
    "    epoch_count += 1\n",
    "    base_dict.clear()\n",
    "    last_batch_ids.clear()\n",
    "\n",
    "    base_dict = {}\n",
    "    while len(result_ids):\n",
    "        done_id, result_ids = ray.wait(result_ids)\n",
    "        dict_id = ray.get(done_id[0])\n",
    "        result_dict = ray.get(dict_id)\n",
    "        merge_dict(base_dict, result_dict)\n",
    "    result_ids.clear()  # clear up the references\n",
    "    dump_dict_2_hdfs_epoch(base_dict, col_names, hdfs_path, fs, epoch_count)\n",
    "    epoch_count += 1\n",
    "    base_dict.clear()\n",
    "    result_ids.clear()\n",
    "\n",
    "    # Merge all the epochs\n",
    "    print(\"= = = Start Merging the Epochs = = =\")\n",
    "    leaves = partition_tree.get_leaves()\n",
    "    pids = [leaf.nid for leaf in leaves]\n",
    "    steps = len(pids) // num_process\n",
    "    not_ready_ids = []\n",
    "    for i in range(num_process):\n",
    "        sub_pids = pids[i * steps:(i + 1) * steps]\n",
    "        if i == num_process - 1:\n",
    "            sub_pids = pids[i * steps:]\n",
    "        rid = merge_epochs.remote([sub_pids, epoch_count, hdfs_path, fs, i])\n",
    "        not_ready_ids.append(rid)\n",
    "\n",
    "    while len(not_ready_ids):\n",
    "        ready_ids, not_ready_ids = ray.wait(not_ready_ids)\n",
    "\n",
    "    ray.shutdown()\n",
    "    # Todo: delete temporary epoch parquet data\n",
    "    print(\"= = = Delete the used Epochs = = =\")\n",
    "    for epoch in range(epoch_count):\n",
    "        del_path=hdfs_path + \"epoch_\" + str(epoch)\n",
    "        if fs.exists(del_path):\n",
    "            fs.rm(del_path,recursive=True)\n",
    "        print(\"delete epoch_\"+str(epoch))\n",
    "    finish_time = time.time()\n",
    "    print('= = = = = TOTAL DATA ROUTING AND PERISITING TIME:', finish_time - begin_time, \"= = = = =\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# = = = Configuration (UBDA Cloud Centos) = = =\n",
    "scale_factor = 1\n",
    "# table_base_path = '/media/datadrive1/TPCH/dbgen/'\n",
    "# table_path = table_base_path + 'lineitem_' + str(scale_factor) + '.tbl'\n",
    "table_path='/home/liupengju/pycharmProjects/NORA_JOIN_SIMULATION/NORA_experiments/dataset/lineitem_1.tbl'\n",
    "\n",
    "num_process = 10\n",
    "chunk_size = 2000000\n",
    "# 6M rows = about 1GB raw data\n",
    "\n",
    "num_dims = 16\n",
    "used_dims = [1, 2, 3]\n",
    "\n",
    "# hdfs_path: whole table data is partitioned into these parquet files.\n",
    "# base path of HDFS\n",
    "hdfs_private_ip = '10.77.110.133'\n",
    "hdfs_base_path = 'hdfs://10.77.110.133:9001/par_nora/'\n",
    "\n",
    "problem_type = 2\n",
    "# nora_hdfs = hdfs_base_path + 'NORA/prob' + str(problem_type) + '/'\n",
    "# qdtree_hdfs = hdfs_base_path + 'QdTree/prob' + str(problem_type) + '/'\n",
    "# kdtree_hdfs = hdfs_base_path + 'KDTree/prob' + str(problem_type) + '/'\n",
    "\n",
    "nora_hdfs = hdfs_base_path + 'NORA/prob' + str(problem_type) + '/scale' + str(scale_factor) + \"/\"\n",
    "qdtree_hdfs = hdfs_base_path + 'QdTree/prob' + str(problem_type) + '/scale' + str(scale_factor) + \"/\"\n",
    "paw_hdfs = hdfs_base_path + 'PAW/prob' + str(problem_type) + '/scale' + str(scale_factor) + \"/\"\n",
    "\n",
    "# === The partition tree files ====\n",
    "# base path of Partition\n",
    "partition_base_path = '/home/liupengju/pycharmProjects/NORA_JOIN_SIMULATION/PartitionLayout/'\n",
    "\n",
    "# partition_path: the location of constructed partition tree\n",
    "# nora_partition = partition_base_path + 'prob' + str(problem_type) + '_nora'\n",
    "# qdtree_partition = partition_base_path + 'prob' + str(problem_type) + '_qdtree'\n",
    "# kdtree_partition = partition_base_path + 'prob' + str(problem_type) + '_kdtree'\n",
    "nora_partition = partition_base_path + 'prob' + str(problem_type) + '_nora_scale' + str(scale_factor)\n",
    "qdtree_partition = partition_base_path + 'prob' + str(problem_type) + '_qdtree_scale' + str(scale_factor)\n",
    "paw_partition = partition_base_path + 'prob' + str(problem_type) + '_paw_scale' + str(scale_factor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch_data_parallel(table_path, nora_partition, chunk_size, used_dims, nora_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "# print('finish nora data routing..')\n",
    "# batch_data_parallel(table_path, paw_partition, chunk_size, used_dims, paw_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "# print('finish paw data routing..')\n",
    "# time.sleep(10000)\n",
    "# batch_data_parallel(table_path, qdtree_partition, chunk_size, used_dims, qdtree_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "# print('finish qdtree data routing..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch_data_parallel(table_path, nora_partition, chunk_size, used_dims, nora_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "# print('finish nora data routing..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch_data_parallel(table_path, paw_partition, chunk_size, used_dims, paw_hdfs, num_dims, num_process, hdfs_private_ip)\n",
    "# print('finish paw data routing..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b76f585cb0017b15e5d8539d3843191cbcb7b2b9a503c1b5935eb73a41b9bd63"
  },
  "kernelspec": {
   "display_name": "Python 3.6.2 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}